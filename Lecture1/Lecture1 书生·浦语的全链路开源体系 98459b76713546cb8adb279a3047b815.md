# Lecture1 书生·浦语的全链路开源体系

<aside>
💡 大模型是发展成为通用人工智能的重要途经，在学术界和工业界都有着非常广泛的应用。书生浦语大模型性能全面领先，被证明是一种非常全面的大模型平台。下面是关于它的全链路体系介绍。

</aside>

### Date: January 9, 2024

### Topic:

<aside>
📢 **全链路体系**

</aside>

![Untitled](Lecture1%20%E4%B9%A6%E7%94%9F%C2%B7%E6%B5%A6%E8%AF%AD%E7%9A%84%E5%85%A8%E9%93%BE%E8%B7%AF%E5%BC%80%E6%BA%90%E4%BD%93%E7%B3%BB%2098459b76713546cb8adb279a3047b815/Untitled.png)

### Recall

**引言**

**书生·浦语大模型是什么？**

**书生·浦语的性能**

**从模型到应用**

**需要的框架/工具**

**全链路是什么？**

**书生万卷数据库2TB**

**OpenDatalab数据库80TB**

**微调工具Xtuner**

**部署工具LMDeploy**

### Notes

- 全球对于大模型关注增多。趋势是专用模型—通用模型。
一个模型解决问题 👉 一个通用的大模型解决多个任务、多个模态（更高阶、更智能）
- 书生·浦语大模型的三个量级：
    - InternLM-7B（可部署）
    - InternLM-20B（可定制）
    - InternLM-123B（能力强）
- 相比其他大模型，以更小的参数达到了更多参数的效果
- 大模型的应用：智能客服、个人助手、行业应用
- 模型的应用到部署
    
    ![Untitled](Lecture1%20%E4%B9%A6%E7%94%9F%C2%B7%E6%B5%A6%E8%AF%AD%E7%9A%84%E5%85%A8%E9%93%BE%E8%B7%AF%E5%BC%80%E6%BA%90%E4%BD%93%E7%B3%BB%2098459b76713546cb8adb279a3047b815/Untitled%201.png)
    
    1. 模型评测：从社区中选择合适的模型（关注不同维度的能力，针对应用场景的能力）
    2. 业务场景是否复杂
    3. 环境交互：调用外部API，和已有业务数据库进行交互

<aside>
📌 **SUMMARY:** 介绍了书生·浦语的全链条开放体系的各个模块，包括**数据、与训练、微调、部署、评测和应用**，每个模块都有很好的性能，同时能和很多平台进行对接，我们能够通过使用已经集成在书生·浦语里的优化算法和相关成型算法，以此为基础进行进一步的大模型开发和研究，最终实现将模型微调、部署并投之于应用。

</aside>

<aside>
❓ **一些其他问题**

</aside>

Xtuner是什么？

是一个微调框架，能够进行全参数的微调和LoRA低成本微调.

能够在8GB的显存下进行7B的模型的微调

![Untitled](Lecture1%20%E4%B9%A6%E7%94%9F%C2%B7%E6%B5%A6%E8%AF%AD%E7%9A%84%E5%85%A8%E9%93%BE%E8%B7%AF%E5%BC%80%E6%BA%90%E4%BD%93%E7%B3%BB%2098459b76713546cb8adb279a3047b815/Untitled%202.png)

OpenCompass是什么？

是评测的工具，可以给一个模型的各个方面评分

什么是智能体？

可以以大模型为基础，进行一些推理和执行策略（网络的搜索、pythonExecutor）

提供了轻量级智能体框架`Lagent`

多模态智能体工具箱`AgentLego`，给大模型提供更多的工具集合

![Untitled](Lecture1%20%E4%B9%A6%E7%94%9F%C2%B7%E6%B5%A6%E8%AF%AD%E7%9A%84%E5%85%A8%E9%93%BE%E8%B7%AF%E5%BC%80%E6%BA%90%E4%BD%93%E7%B3%BB%2098459b76713546cb8adb279a3047b815/Untitled%203.png)

为什么要微调？

微调大模型可以使其更好地适应特定任务，提高性能，并且可以在小样本情况下利用预训练模型的知识。

HuggingFace是什么？

HuggingFace也是一个开源的自然语言处理（NLP）模型库和平台，为开发者提供了一系列用于构建、训练和部署NLP模型的工具和资源。它的目标是使NLP模型的开发变得更加简单、高效和可复用。HuggingFace还维护了一个名为"Transformers"的开源社区，该社区汇集了全球的研究人员和开发者，共同贡献和分享NLP模型、工具和资源。

---